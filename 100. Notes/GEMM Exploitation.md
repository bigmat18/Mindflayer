**Data time:** 20:40 - 01-06-2025

**Status**: #note #youngling 

**Tags:** [[High Performance Computing]] [[CUDA Memory Model]]

**Area**: [[Master's degree]]
# GEMM Exploitation

Let us consider the **[[Matrix Multiplication in CUDA|matrix multiplicatio]]** n problem with the na√Øve CUDA implementation (assume the row-major layout)

```c
unsigned int iy = (blockIdx.y * blockDim.y) + threadIdx.y;
unsigned int ix = (blockIdx.x * blockDim.x) + threadIdx.x;
if (iy < L && ix < L) {
	float sum = 0.0f;
	for (unsigned int k=0; k<L; k++) {
		sum += A[iy * L + k] * B[k * L + ix];
	}
	C[iy * L + ix] = sum;
}
```

Threads in **warp 0** access element $A_{0,0}$ when $k=0$ (**one transaction**), and they access (all in parallel) the elements of B in the same row (**second transaction**).

![[Pasted image 20250601204304.png]]

Accesses to GMEM are **coalesced** owing to the row-major layout of the two matrices.

### [[Matrix Transponse in CUDA]]

### [[Data Layout in GEMM]]

### [[Thread Coarsening]]


### Optimization Recap

![[Pasted image 20250601214309.png]]

# References